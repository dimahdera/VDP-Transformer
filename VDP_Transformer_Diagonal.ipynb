{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmgq8PxqWRB8zJ7zaX/m0s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xc__xVMuPGyx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from tensorflow.keras.layers import ( Dense, Dropout,  LayerNormalization,)\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "# For multiple devices (GPUs: 4, 5, 6, 7)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,4,5,6,7\"\n",
        "# import imageio\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import time, sys\n",
        "import pickle\n",
        "import timeit\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras import layers\n",
        "#import tensorflow_addons as tfa\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import pandas as pd\n",
        "#import wandb\n",
        "#os.environ[\"WANDB_API_KEY\"] = \"key_code\"\n",
        "\n",
        "import numpy as np\n",
        "#!pip install tensorflow_addons\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from tensorflow.keras import layers\n",
        "#import tensorflow_addons as tfa\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import pandas as pd\n",
        "plt.ioff()\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# update_progress() : Displays or updates a console progress bar\n",
        "## Accepts a float between 0 and 1. Any int will be converted to a float.\n",
        "## A value under 0 represents a 'halt'.\n",
        "## A value at 1 or bigger represents 100%\n",
        "def update_progress(progress):\n",
        "    barLength = 10  # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    if not isinstance(progress, float):\n",
        "        progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if progress < 0:\n",
        "        progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if progress >= 1:\n",
        "        progress = 1\n",
        "        status = \"Done...\\r\\n\"\n",
        "    block = int(round(barLength * progress))\n",
        "    text = \"\\rPercent: [{0}] {1}% {2}\".format(\"#\" * block + \"-\" * (barLength - block), progress * 100, status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "# class Patches(tf.keras.layers.Layer):\n",
        "#     def __init__(self, patch_size):\n",
        "#         super(Patches, self).__init__()\n",
        "#         self.patch_size = patch_size\n",
        "\n",
        "#     def call(self, images):\n",
        "#         batch_size = tf.shape(images)[0]\n",
        "#         patches = tf.image.extract_patches(\n",
        "#             images=images,\n",
        "#             sizes=[1, self.patch_size, self.patch_size,  1],\n",
        "#             strides=[1, self.patch_size, self.patch_size, 1],\n",
        "#             rates=[1, 1,1, 1],\n",
        "#             padding=\"VALID\",\n",
        "#         )\n",
        "#         patch_dims = patches.shape[-1]\n",
        "#         patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "#         return patches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Auxiliary Functions**"
      ],
      "metadata": {
        "id": "fpHXyb1fyfXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def x_Sigma_w_x_T(x, W_Sigma):\n",
        "  batch_sz = x.shape[0]\n",
        "  xx_t = tf.reduce_sum(tf.multiply(x, x),axis=-1, keepdims=True)          #[50, 17, 64]  -> [50, 17, 1] or [50, 64] - > [50, 1]\n",
        " # xx_t_e = tf.expand_dims(xx_t,axis=2)                                   \n",
        "  return tf.multiply(xx_t_e, W_Sigma) #[50,17,64] or [50, 64] or [50, 10]\n",
        "\n",
        "def w_t_Sigma_i_w(w_mu, in_Sigma):  #[64, 64]  , [50, 17, 64] or [64, 10], [50, 64]\n",
        "  Sigma_1 = tf.matmul(in_Sigma, tf.multiply(w_mu, w_mu))      #[50, 17, 64] or [50, 10]\n",
        "  return Sigma_1\n",
        "\n",
        "def tr_Sigma_w_Sigma_in(in_Sigma, W_Sigma):\n",
        "  Sigma = tf.reduce_sum(in_Sigma, axis=-1, keepdims=True ) #[50,17, 1]\n",
        "  return tf.multiply(Sigma, W_Sigma)  #[50,17, 64]\n",
        "\n",
        "\n",
        "# def sigma_regularizer1(x):\n",
        "#     input_size = 1.0   \n",
        "#     f_s = tf.math.softplus(x) #tf.math.log(1. + tf.math.exp(x)) \n",
        "#     return  input_size * tf.reduce_mean(1. + tf.math.log(f_s) - f_s )"
      ],
      "metadata": {
        "id": "mAHArl3vyemc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Dense Layers**"
      ],
      "metadata": {
        "id": "POEj-FSJ7Psu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "    def __init__(self, units):\n",
        "        super(LinearFirst, self).__init__()\n",
        "        self.units = units\n",
        "    def build(self, input_shape):\n",
        "        tau = -1. /input_shape[-1]\n",
        "        self.w_mu = self.add_weight(name='w_mu',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),regularizer=tf.keras.regularizers.l2(tau)\n",
        "            trainable=True        )\n",
        "        self.w_sigma = self.add_weight(name='w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer=tf.random_uniform_initializer(minval= -12., maxval=-2.2, seed=None), regularizer=sigma_regularizer2,\n",
        "            trainable=True        )\n",
        "\n",
        "    def call(self, inputs):# [50,17,64]\n",
        "        # Mean\n",
        "        #print(self.w_mu.shape)\n",
        "        mu_out = tf.matmul(inputs, self.w_mu) #+ self.b_mu       [50, 17, 64]             # Mean of the output\n",
        "        # Varinace\n",
        "        W_Sigma = tf.math.log(1. + tf.math.exp(self.w_sigma))         #[64]                        # Construct W_Sigma from w_sigmas\n",
        "        Sigma_out = x_Sigma_w_x_T(inputs, W_Sigma)# [50, 17, 64]            + tf.math.log(1. + tf.math.exp(self.b_sigma)) #tf.linalg.diag(self.b_sigma)\n",
        "        Sigma_out = tf.where(tf.math.is_nan(Sigma_out), tf.zeros_like(Sigma_out), Sigma_out)\n",
        "        Sigma_out = tf.where(tf.math.is_inf(Sigma_out), tf.zeros_like(Sigma_out), Sigma_out)     \n",
        "        Sigma_out = tf.abs(Sigma_out)      \n",
        "        return mu_out, Sigma_out\n",
        "\n",
        "\n",
        "class LinearNotFirst(keras.layers.Layer):\n",
        "    \"\"\"y = w.x + b\"\"\"\n",
        "    def __init__(self, units):\n",
        "        super(LinearNotFirst, self).__init__()\n",
        "        self.units = units\n",
        "                \n",
        "    def build(self, input_shape):\n",
        "        ini_sigma = -2.2\n",
        "        #min_sigma = -4.5\n",
        "        tau = -1. /input_shape[-1]    \n",
        "        \n",
        "        self.w_mu = self.add_weight(name = 'w_mu', shape=(input_shape[-1], self.units),   #[64 , 64] or or [64, 10] or [10, 10]\n",
        "            initializer=tf.random_normal_initializer( mean=0.0, stddev=0.05, seed=None), regularizer=tf.keras.regularizers.l2(tau),#tau/self.units), #tf.keras.regularizers.l2(0.5*0.001),\n",
        "            trainable=True,        )\n",
        "        self.w_sigma = self.add_weight(name = 'w_sigma',\n",
        "            shape=(self.units,),\n",
        "            initializer= tf.constant_initializer(ini_sigma),#tf.random_uniform_initializer(minval= min_sigma, maxval=ini_sigma, seed=None) , \n",
        "            regularizer=sigma_regularizer2, #   tf.constant_initializer(ini_sigma)\n",
        "            trainable=True,        )   \n",
        "    def call(self, mu_in, Sigma_in):     # [50,17,64],  [50,17,64]   or [50, 64] or [50, 10]\n",
        "        mu_out = tf.matmul(mu_in, self.w_mu) #+ self.b_mu  [50, 17, 64] \n",
        "\n",
        "        W_Sigma = tf.math.log(1. + tf.math.exp(self.w_sigma))    # [64] \n",
        "        Sigma_1 = w_t_Sigma_i_w (self.w_mu, Sigma_in) [50,17,64]\n",
        "        Sigma_2 = x_Sigma_w_x_T(mu_in, W_Sigma)    # [50, 17, 64]                              \n",
        "        Sigma_3 = tr_Sigma_w_Sigma_in (Sigma_in, W_Sigma) #[50, 17, 64]\n",
        "        Sigma_out = Sigma_1 + Sigma_2 + Sigma_3 #+ tf.linalg.diag(tf.math.log(1. + tf.math.exp(self.b_sigma)))  #[50, 17, 64]\n",
        "        \n",
        "        Sigma_out = tf.where(tf.math.is_nan(Sigma_out), tf.zeros_like(Sigma_out), Sigma_out)  #[50,2,17,64,64]\n",
        "        Sigma_out = tf.where(tf.math.is_inf(Sigma_out), tf.zeros_like(Sigma_out), Sigma_out)  #[50,2,17,64,64]\n",
        "        Sigma_out = tf.abs(Sigma_out) \n",
        "        return mu_out, Sigma_out   # mu_out=[50,17,64], Sigma_out = [50,17,64] "
      ],
      "metadata": {
        "id": "zwDCV70YQMs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Activation Functions**"
      ],
      "metadata": {
        "id": "LPjZ-hTD7VaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VDP_GeLU(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(VDP_GeLU, self).__init__()\n",
        "    def call(self, mu_in,Sigma_in):   #mu_in = [50,17,64], Sigma_in= [50,17,64]\n",
        "        mu_out = tf.nn.gelu(mu_in)  #[50,17,64]\n",
        "        with tf.GradientTape() as g:\n",
        "            g.watch(mu_in)\n",
        "            out = tf.nn.gelu(mu_in) \n",
        "        gradi = g.gradient(out, mu_in) #[50,17,64]\n",
        "        Sigma_out = activation_sigma(gradi, Sigma_in)\n",
        "        return mu_out, Sigma_out  #[50,2,17,64], [50,2,17,64,64]\n",
        "\n",
        "\n",
        "def activation_Sigma (gradi, Sigma_in):\n",
        "  grad1 = tf.multiply(gradi,gradi) #[50,17,64] or [50, 10]\n",
        "  return tf.multiply(Sigma_in, grad1) #[50,17,64] or [50, 10]\n",
        "\n",
        "\n",
        " class VDP_ReLU(keras.layers.Layer):\n",
        "    \"\"\"ReLU\"\"\"\n",
        "    def __init__(self):\n",
        "        super(VDP_ReLU, self).__init__()\n",
        "    def call(self, mu_in, Sigma_in):\n",
        "        mu_out = tf.nn.relu(mu_in)\n",
        "        with tf.GradientTape() as g:\n",
        "          g.watch(mu_in)\n",
        "          out = tf.nn.relu(mu_in)\n",
        "        gradi = g.gradient(out, mu_in) \n",
        "        Sigma_out = activation_Sigma(gradi, Sigma_in)        \n",
        "        return mu_out, Sigma_out"
      ],
      "metadata": {
        "id": "R6uGLF4e7VFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Dropout**"
      ],
      "metadata": {
        "id": "K3TayaKmEJ4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VDP_Dropout(keras.layers.Layer):\n",
        "  def __init__(self, drop_prop):\n",
        "    super(VDP_Dropout, self).__init__()\n",
        "    self.drop_prop = drop_prop\n",
        "  def call(self, mu_in,Sigma_in, Training=True):\n",
        "    # shape=[batch_size, seq length, embedding_dim]\n",
        "    scale_sigma = 1.0 / (1 - self.drop_prop)\n",
        "    if Training:\n",
        "      mu_out = tf.nn.dropout(mu_in, rate=self.drop_prop) #[50,17,64] or [50, 10]\n",
        "      non_zero = tf.not_equal(mu_out, tf.zeros_like(mu_out))#[50,17,64]\n",
        "      non_zero_sigma_mask = tf.boolean_mask(Sigma_in, non_zero) \n",
        "      idx_sigma = tf.dtypes.cast(tf.where(non_zero), tf.int32)\n",
        "      Sigma_out =  (scale_sigma**2) * tf.scatter_nd(idx_sigma, non_zero_sigma_mask, tf.shape(non_zero)) \n",
        "    else:\n",
        "      mu_out = mu_in\n",
        "      Sigma_out = Sigma_in\n",
        "    return mu_out,Sigma_out  #[50,17,64], [50,17,64]"
      ],
      "metadata": {
        "id": "-CDUganBEI46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian MLP**"
      ],
      "metadata": {
        "id": "jt0eXo-Yx_tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VDP_MLP(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_features, out_features, dropout_rate=0.1):\n",
        "        super(VDP_MLP, self).__init__()\n",
        "        self.dense1 = LinearNotFirst(hidden_features)\n",
        "        self.dense2 = LinearNotFirst(out_features)\n",
        "        self.dropout1 =VDP_Dropout(dropout_rate)\n",
        "        self.gelu_1 = VDP_GeLU()\n",
        "    def call(self, mu_in, sigma_in):\n",
        "        mu_out, sigma_out = self.dense1(mu_in, sigma_in)\n",
        "    #    print('shape of x(MLP layer) :',x.shape)\n",
        "        mu_out, sigma_out = self.gelu_1(mu_out, sigma_out)\n",
        "     #   print('shape of x through GeLU :',x.shape)\n",
        "        mu_out, sigma_out = self.dropout1(mu_out, sigma_out)\n",
        "    #    print('shape of x after dropout :',x.shape)\n",
        "        mu_out, sigma_out = self.dense2(mu_out, sigma_out)\n",
        "    #    print('shape of x after 2nd dense :',x.shape)\n",
        "        mu_out, sigma_out = self.dropout1(mu_out, sigma_out)\n",
        "     #   print('shape of y', y.shape)\n",
        "        return mu_out, sigma_out"
      ],
      "metadata": {
        "id": "1anttwnUx-ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deterministic Layer Norm**"
      ],
      "metadata": {
        "id": "XofFP8mcBkr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(tf.keras.layers.Layer):\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "      self.eps = eps\n",
        "      super(LayerNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                   initializer=tf.keras.initializers.Ones(), trainable=True)\n",
        "      self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                  initializer=tf.keras.initializers.Zeros(), trainable=True)\n",
        "      super(LayerNorm, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "      mean = K.mean(x, axis=-1, keepdims=True)\n",
        "      std = K.std(x, axis=-1, keepdims=True)\n",
        "      #print( \"mean of LN\",mean.shape)\n",
        "      #print(\"std of LN\",std.shape)\n",
        "      return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "      return input_shape"
      ],
      "metadata": {
        "id": "JQBat7eNBjYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Layernorm**"
      ],
      "metadata": {
        "id": "UjocMiL9RkJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bayesian_LayerNorm(layers.Layer):\n",
        "\n",
        "    def __init__(self, eps=1e-6, **kwargs):\n",
        "        self.eps = eps\n",
        "        super(Bayesian_LayerNorm, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
        "                                     initializer=tf.keras.initializers.Ones(), trainable=True)\n",
        "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
        "                                    initializer=tf.keras.initializers.Zeros(), trainable=True)\n",
        "        super(Bayesian_LayerNorm, self).build(input_shape)\n",
        "    def call(self, mu_x, sigma_x):  # (batch_size, sequence_length, embedding_dim),  (batch_size, sequence_length, embedding_dim)[50,17, 64],[50,17,64]\n",
        "        mean = K.mean(mu_x, axis=-1, keepdims=True) #[50,17,1]\n",
        "        std = K.std(mu_x, axis=-1, keepdims=True) #[50,17,1]\n",
        "       # print('std = ' , std.shape)\n",
        "      #  print('gamma = ',self.gamma)\n",
        "        out_mu= self.gamma * (mu_x - mean) / (std + self.eps) + self.beta\n",
        "        a = (self.gamma/(std+self.eps))**2 #[50,17,64]\n",
        "  \n",
        "        out_sigma = tf.math.multiply(a , sigma_x)   #[50,17,64]\n",
        "        return out_mu, out_sigma\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "metadata": {
        "id": "KvZc7soXRomf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Multi-Head Attention**"
      ],
      "metadata": {
        "id": "DouuToSeRxd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bayesian_MultiHeadSelfAttention_First(tf.keras.layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads=8):\n",
        "    super(Bayesian_MultiHeadSelfAttention_First, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    if embed_dim % num_heads != 0:\n",
        "      raise ValueError(\n",
        "        f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "      )\n",
        "    self.projection_dim = embed_dim // num_heads\n",
        "    self.query_dense = LinearFirst(embed_dim)\n",
        "    self.key_dense = LinearFirst(embed_dim)\n",
        "    self.value_dense = LinearFirst(embed_dim)\n",
        "    self.combine_heads = LinearFirst(embed_dim)\n",
        "\n",
        "  def attention(self, mu_query,sigma_query, mu_key,sigma_key, mu_value,sigma_value, input_dimension):\n",
        "    mu_score = tf.matmul(mu_query, mu_key, transpose_b=True) # [50, 2, 17, 32] x [50, 2, 32, 17] = [50, 2, 17, 17]\n",
        "\n",
        "    a = tf.reduce_sum(tf.math.multiply(mu_query**2 , sigma_key), axis=-1, keepdims=True) # [50, 2, 17, 1] \n",
        "    b = tf.transpose(tf.reduce_sum(tf.math.multiply(mu_key**2 , sigma_query), axis=-1, keepdims=True), perm=[0, 1, 3, 2])# [50, 2, 1, 17 ]\n",
        "    a_b= a + b # [50, 2, 17, 17] \n",
        "\n",
        "    \n",
        "    c1 = tf.reduce_sum(tf.math.multiply(sigma_query, sigma_key), axis=-1, keepdims=True) # [50, 2, 17, 1]\n",
        "    c2 = tf.transpose(c1, perm=[0, 1, 3, 2] ) # [50, 2, 1, 17]\n",
        "    c = c1 + c2 # [50, 2, 17, 17]     \n",
        "    sigma_score = a_b + c # [50, 2, 17, 17]\n",
        "    dim_key = tf.cast(tf.shape(mu_key)[-1], tf.float32)\n",
        "    mu_scaled_score = mu_score / tf.math.sqrt(dim_key) #[50, 2, 17, 17]\n",
        "    sigma_scaled_score = sigma_score * dim_key # [50, 2, 17, 17]\n",
        "\n",
        "    mu_weights = tf.nn.softmax(mu_scaled_score, axis=-1) # [50, 2, 17, 17]\n",
        "    # Sigma for softmax function\n",
        "    pp1 = tf.expand_dims(mu_weights, axis=-1) # [50, 2, 17, 17,1]\n",
        "    pp2 = tf.expand_dims(mu_weights, axis=3) # [50, 2, 17,1, 17]\n",
        "    ppT = tf.matmul(pp1, pp2) # # [50, 2, 17, 17,17]\n",
        "    p_diag = tf.linalg.diag(mu_weights) # [50, 2, 17, 17,17]\n",
        "    grad = (p_diag - ppT)**2 # # [50, 2, 17, 17,17]\n",
        "    Sigma_weights = tf.squeeze(tf.matmul(grad, tf.expand_dims(sigma_scaled_score, axis=-1))) # [50, 2, 17, 17]\n",
        "    Sigma_weights = tf.where(tf.math.is_nan(Sigma_out), tf.zeros_like(Sigma_out), Sigma_ou\n",
        "    Sigma_weights = tf.where(tf.math.is_inf(Sigma_out), tf.zeros_like(Sigma_out), Sigma_out)\n",
        "   # Sigma_weights = tf.linalg.set_diag(Sigma_out, tf.abs(tf.linalg.diag_part(Sigma_out)))\n",
        "\n",
        "    mu_output = tf.matmul(mu_weights, mu_value)  #[50,2,17,17] X [50,2,17,32]=  [50,2,17,32] \n",
        "    d = tf.matmul(mu_weights**2,sigma_value)) #[50,2,17,32] \n",
        "    e = tf.matmul(Sigma_weights, mu_value**2)) #[50,2,17,32] \n",
        "    f = tf.matmul(Sigma_weights, sigma_value) # [50, 2, 17, 17]x[50,2,17,32]=  [50,2,17,32] \n",
        "    output_sigma = d + e + f     \n",
        "    return mu_output, sigma_output #, mu_weights, Sigma_weights\n",
        "\n",
        "  def separate_heads(self, mu_x, sigma_x batch_size): # [50, 17,64], [50, 17, 64]\n",
        "    mu_x = tf.reshape(mu_x, (batch_size, -1, self.num_heads, self.projection_dim)) # [50, 17, 2 ,32]\n",
        "    sigma_x = tf.reshape(sigma_x, (batch_size, -1, self.num_heads, self.projection_dim)) # [50, 17, 2 32]\n",
        "    mu_x = tf.transpose(mu_x, perm=[0, 2, 1, 3]) # [50, 2, 17, 32]\n",
        "    sigma_x = tf.transpose(sigma_x, perm=[0, 2, 1, 3]) # [50, 2, 17, 32]\n",
        "    return mu_x, sigma_x #[50,2,17,32],[50,2,17,32]\n",
        "\n",
        "  def call(self, inputs):\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "    mu_query, sigma_query = self.query_dense(inputs)  # [50, 17,64]  , [50, 17,64] \n",
        "    mu_key, sigma_key = self.key_dense(inputs)   # [50, 17,64] , [50, 17,64] \n",
        "    mu_value, sigma_value = self.value_dense(inputs) # [50, 17,64], [50, 17,64] \n",
        "\n",
        "    mu_query,sigma_query = self.separate_heads(mu_query,sigma_query, batch_size)\n",
        "    mu_key, sigma_key = self.separate_heads(mu_key, sigma_key, batch_size)\n",
        "    mu_value,sigma_value = self.separate_heads(mu_value,sigma_value, batch_size)\n",
        "\n",
        "    mu_attention, sigma_attention = self.attention(mu_query,sigma_query, mu_key, sigma_key, mu_value,sigma_value, tf.shape(inputs)[1])\n",
        "    mu_attention = tf.transpose(mu_attention, perm=[0, 2, 1, 3])\n",
        "    sigma_attention = tf.transpose(sigma_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    mu_concat_attention = tf.reshape(mu_attention, (batch_size, -1, self.embed_dim))\n",
        "    sigma_concat_attention = tf.reshape(sigma_attention, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "    mu_output = self.combine_heads(mu_concat_attention)\n",
        "    sigma_output = self.combine_heads(sigma_concat_attention)\n",
        "    return mu_output, sigma_output"
      ],
      "metadata": {
        "id": "RSsG5WEZRsPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Transformer Block**"
      ],
      "metadata": {
        "id": "mMLTgu0RQRPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VDP_TransformerBlock_first(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
        "        super(VDP_TransformerBlock_first, self).__init__()\n",
        "        self.att = Bayesian_MultiHeadSelfAttention_First(embed_dim, num_heads)  #[64,2]\n",
        "        self.mlp = VDP_MLP(mlp_dim*2,mlp_dim,dropout) #[64*2,64,dropout]\n",
        "        self.layernorm1 = LayerNorm(eps=1e-6)\n",
        "        self.layernorm2 = Bayesian_LayerNorm(eps=1e-6)\n",
        "        self.dropout1 = VDP_Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        inputs_norm = self.layernorm1(inputs) #[50,17,64]\n",
        "        mu_output , sigma_out = self.att(inputs_norm) #[50,17,64]\n",
        "        mu_output , sigma_out1 = self.dropout1(mu_output , sigma_out, training=training)#[50,17,64]\n",
        "        mu_out1 = mu_output + inputs  #[50,17,64]\n",
        "\n",
        "        mu_out1_norm, sigma_out1_norm = self.layernorm2(mu_out1, sigma_out1)\n",
        "        mu_mlp_output, sigma_mlp_output = self.mlp(mu_out1_norm, sigma_out1_norm)\n",
        "        mu_mlp_output, sigma_mlp_output = self.dropout1(mu_mlp_output, sigma_mlp_output, training=training)\n",
        "        mu_output = mu_mlp_output + mu_out1\n",
        "        with tf.GradientTape() as g:\n",
        "             g.watch(mu_out1)\n",
        "             out = mu_mlp_output + mu_out1\n",
        "        gradi = g.gradient(out, mu_out1) \n",
        "        sigma_output = tf.math.multiply(tf.math.multiply(gradi, gradi), sigma_out1)\n",
        "        return mu_output, sigma_output"
      ],
      "metadata": {
        "id": "upka16TqphIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bayesian Transformer using Uncertainty Propagation Framework**"
      ],
      "metadata": {
        "id": "0Ctp-k4V7Xg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VDP_ViT(tf.keras.Model):\n",
        "  def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        num_layers,\n",
        "        num_classes,\n",
        "        d_model,\n",
        "        num_heads,\n",
        "        mlp_dim,\n",
        "        channels=1,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "      super(VDP_ViT, self).__init__()\n",
        "      num_patches = (image_size // patch_size) ** 2\n",
        "      self.patch_dim = channels * (patch_size ** 2)\n",
        "\n",
        "      self.patch_size = patch_size\n",
        "      self.d_model = d_model\n",
        "      self.num_layers = num_layers\n",
        "\n",
        "      self.rescale = Rescaling(1.0 / 255)\n",
        "      self.pos_emb = self.add_weight(\n",
        "        \"pos_emb\", shape=(1, num_patches + 1, d_model)\n",
        "        )\n",
        "      self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\n",
        "      self.patch_proj = DDense(d_model)\n",
        "      self.enc_layers = VDP_TransformerBlock_first(d_model, num_heads, mlp_dim, dropout))\n",
        "     # self.enc_layers = [ VDP_TransformerBlock(d_model, num_heads, mlp_dim, dropout)\n",
        "     #   for _ in range(num_layers)\n",
        "     #   ]\n",
        "      self.mlp_head =VDP_MLP(mlp_dim, num_classes)\n",
        "  def extract_patches(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1,self.patch_size, self.patch_size, 1],\n",
        "            strides=[1,self.patch_size, self.patch_size, 1],\n",
        "            rates=[1,1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\n",
        "        return patches\n",
        "\n",
        "  def call(self, x, training):\n",
        "        print('Input dimension :',x.shape)\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = self.rescale(x)\n",
        "        print('Input dimension after rescale :',x.shape)\n",
        "        patches = self.extract_patches(x)\n",
        "        print('Input dimension after extract patch :',patches.shape)\n",
        "        x = self.patch_proj(patches)\n",
        "        print('Input dimension after patch projection :',x.shape)\n",
        "        class_emb = tf.broadcast_to(\n",
        "        self.class_emb, [batch_size, 1, self.d_model]\n",
        "        )\n",
        "        x = tf.concat([class_emb, x], axis=1)\n",
        "        print('Input dimension after concat :',x.shape)\n",
        "        x = x + self.pos_emb\n",
        "        print('Input dimension after x = x + self.pos_emb :',x.shape)\n",
        "        mu_out, sigma_out = self.enc_layers(x)\n",
        "   #     for layer in self.enc_layers:\n",
        "   #         x = layer(x, training)\n",
        "\n",
        "    # First (class token) is used for classification\n",
        "        mu, sigma = self.mlp_head(mu_out[:, 0], sigma_out[:, 0] )\n",
        "        return mu, sigma"
      ],
      "metadata": {
        "id": "-ZJlf5Ok-bR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function for the Bayesian Framework**"
      ],
      "metadata": {
        "id": "zqjmNJ8eeHWq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqvKxkFzeHEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_function(image_size=28,patch_size=7,num_layers=2,num_classes=10,d_model=64,num_heads=2,mlp_dim=64,channels=1,\n",
        "                  dropout=0.1,\n",
        "                  batch_size=50, epochs=10, lr=0.001, lr_end = 0.0001,\n",
        "                  Targeted=False,\n",
        "                Training=True, continue_training=False, saved_model_epochs=10):\n",
        "    #PATH = './saved_models/cnn_epoch_{}/'.format(epochs)\n",
        "    # the data, split between train and test sets\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Scale images to the [0, 1] range\n",
        "    x_train = x_train.astype(\"float32\") / 255\n",
        "    x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "    # Make sure images have shape (28, 28, 1)\n",
        "    x_train = np.expand_dims(x_train, -1)\n",
        "    x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "    trans_model = VDP_ViT(image_size=image_size, patch_size=patch_size, num_layers=num_layers,\n",
        "                                  num_classes=num_classes, d_model= d_model, num_heads=num_heads,mlp_dim = mlp_dim,\n",
        "                                  channels= channels)\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "    tr_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "    num_train_steps = epochs * int(x_train.shape[0] / batch_size)\n",
        "    #    step = min(step, decay_steps)\n",
        "    #    ((initial_learning_rate - end_learning_rate) * (1 - step / decay_steps) ^ (power) ) + end_learning_rate\n",
        "    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=lr,\n",
        "                                                                     decay_steps=num_train_steps,\n",
        "                                                                     end_learning_rate=lr_end, power=2.)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)  # , clipnorm=1.0)\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    \n",
        "\n",
        "    @tf.function  # Make it fast.\n",
        "    def train_on_batch(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            trans_model.trainable = True\n",
        "            out = trans_model(x, training=True)\n",
        "            loss = loss_fn(y, out)\n",
        "            gradients = tape.gradient(loss, trans_model.trainable_weights)\n",
        "\n",
        "            #  gradients = [(tf.where(tf.math.is_nan(grad), tf.constant(1.0e-5, shape=grad.shape), grad)) for grad in gradients]\n",
        "        #  gradients = [(tf.where(tf.math.is_inf(grad), tf.constant(1.0e-5, shape=grad.shape), grad)) for grad in gradients]\n",
        "        optimizer.apply_gradients(zip(gradients, trans_model.trainable_weights))\n",
        "        \n",
        "        return loss, out\n",
        "\n",
        "    @tf.function\n",
        "    def validation_on_batch(x, y):\n",
        "        trans_model.trainable = False\n",
        "        out = trans_model(x, training=False)\n",
        "        total_vloss = loss_fn(y, out)\n",
        "        return total_vloss, out\n",
        "\n",
        "    @tf.function\n",
        "    def test_on_batch(x, y):\n",
        "        trans_model.trainable = False\n",
        "        out = trans_model(x, training=False)\n",
        "        return out\n",
        "    if Training: \n",
        "       # wandb.init(entity = \"dimah\", project=\"DCNN_Cifar10_11layers_epochs_{}_lr_{}_latest\".format(epochs, lr)) \n",
        "        \n",
        "        if continue_training:\n",
        "            saved_model_path = './saved_models/cnn_epoch_{}/'.format(saved_model_epochs)\n",
        "            trans_model.load_weights(saved_model_path + 'Deterministic_cnn_model')\n",
        "        \n",
        "        train_acc = np.zeros(epochs)\n",
        "        valid_acc = np.zeros(epochs)        \n",
        "        train_err = np.zeros(epochs)\n",
        "        valid_err = np.zeros(epochs)                      \n",
        "        start = timeit.default_timer()\n",
        "        for epoch in range(epochs):\n",
        "            print('Epoch: ', epoch + 1, '/', epochs)            \n",
        "            tr_no_steps = 0\n",
        "            va_no_steps = 0\n",
        "            # -------------Training--------------------\n",
        "            acc_training = np.zeros(int(x_train.shape[0] / (batch_size)))\n",
        "            err_training = np.zeros(int(x_train.shape[0] / (batch_size)))            \n",
        "            for step, (x, y) in enumerate(tr_dataset):\n",
        "                update_progress(step / int(x_train.shape[0] / (batch_size)))\n",
        "                loss, out = train_on_batch(x, y)             \n",
        "                err_training[tr_no_steps] = loss.numpy()              \n",
        "                corr = tf.equal(tf.math.argmax(out, axis=-1), tf.math.argmax(y, axis=-1))\n",
        "                accuracy = tf.reduce_mean(tf.cast(corr, tf.float32))                  \n",
        "                acc_training[tr_no_steps] = accuracy.numpy()                                               \n",
        "                tr_no_steps += 1                \n",
        "              \n",
        "            train_acc[epoch] = np.mean(np.amax(acc_training))\n",
        "            train_err[epoch] = np.mean(np.amin(err_training))\n",
        "            print('Training Acc  ', train_acc[epoch])\n",
        "            print('Training error', train_err[epoch])       \n",
        "            # ---------------Validation----------------------  \n",
        "            acc_validation = np.zeros(int(x_test.shape[0] / (batch_size)))\n",
        "            err_validation = np.zeros(int(x_test.shape[0] / (batch_size)))                     \n",
        "            for step, (x, y) in enumerate(val_dataset):\n",
        "                update_progress(step / int(x_test.shape[0] / (batch_size)))\n",
        "                total_vloss, out = validation_on_batch(x, y)                   \n",
        "                err_validation[va_no_steps] = total_vloss.numpy()              \n",
        "                corr = tf.equal(tf.math.argmax(out, axis=-1), tf.math.argmax(y, axis=-1))\n",
        "                va_accuracy = tf.reduce_mean(tf.cast(corr, tf.float32))                 \n",
        "                acc_validation[va_no_steps] = va_accuracy.numpy()                \n",
        "                va_no_steps += 1               \n",
        "            \n",
        "            valid_acc[epoch] = np.mean(np.amax(acc_validation))\n",
        "            valid_err[epoch] = np.mean(np.amin(err_validation))           \n",
        "            stop = timeit.default_timer() \n",
        "            #cnn_model.save_weights(PATH + 'Deterministic_cnn_model')                   \n",
        "##            wandb.log({\"Training Loss\":  train_err[epoch],                        \n",
        "##                       \"Training Accuracy\": train_acc[epoch],                                             \n",
        "##                        \"Validation Loss\": valid_err[epoch],                        \n",
        "##                        \"Validation Accuracy\": valid_acc[epoch],                       \n",
        "##                        'epoch': epoch\n",
        "##                       })             \n",
        "            print('Total Training Time: ', stop - start)\n",
        "            print(' Training Acc   ', train_acc[epoch])            \n",
        "            print(' Validation Acc ', valid_acc[epoch])            \n",
        "            print('------------------------------------')\n",
        "            print('Training error   ', train_err[epoch])            \n",
        "            print('Validation error', valid_err[epoch])                    \n",
        "            # -----------------End Training--------------------------                               \n",
        "           \n",
        "       \n",
        "if __name__ == '__main__':\n",
        "    main_function()\n",
        "\n"
      ],
      "metadata": {
        "id": "jiW8ve00QGvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}